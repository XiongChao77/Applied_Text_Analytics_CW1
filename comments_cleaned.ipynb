{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Google API client to fetch data (YouTube comments)\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Sentiment analysis library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fetch YouTube Video IDs Using Google API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script leverages the YouTube Data API v3 to perform programmatic video searches for specified queries, retrieving and aggregating the videoId of each result. It ensures a unique set of video IDs, forming a foundation for subsequent operations like comment scraping or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up YouTube API Client\n",
    "API_KEY = \"AIzaSyCXQAtU3J_UDEt2XtY0NC_XJXP4SvDIJso\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Function: Search Videos\n",
    "def search_videos(query, max_results=15):\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        q=query,\n",
    "        type=\"video\",\n",
    "        maxResults=max_results,\n",
    "        order=\"relevance\"\n",
    "    )\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    video_ids = []\n",
    "\n",
    "    for item in response[\"items\"]:\n",
    "        video_ids.append(item[\"id\"][\"videoId\"])\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Define queries\n",
    "queries = [\n",
    "    \"Amazon work culture\",\n",
    "    \"why I quit job at Amazon\",\n",
    "    \"Amazon employee review\",\n",
    "    \"Amazon toxic workplace\"\n",
    "]\n",
    "\n",
    "video_ids = []\n",
    "\n",
    "# Fetch videos for all queries\n",
    "for query in queries:\n",
    "    ids = search_videos(query, max_results=30)\n",
    "    video_ids.extend(ids)\n",
    "\n",
    "video_ids = list(set(video_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Fetch Comments from a YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calls **YouTube CommentThreads API** to fetch top-level comments for a given videoId, paginating through results using list_next() until reaching max_comments. It handles errors gracefully if comments are disabled or restricted, returning a **list of plain-text comments** for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_video(video_id, max_comments=200):\n",
    "    comments = []\n",
    "\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "\n",
    "        while request and len(comments) < max_comments:\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comments.append(comment)\n",
    "\n",
    "            request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Skipping video {video_id} (comments disabled or restricted)\")\n",
    "        return []\n",
    "\n",
    "    return comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Aggregate Comments from Multiple Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over all fetched `video_ids`, calling `get_comments_from_video()` for each and storing the results with their corresponding video IDs in a **structured list**. It then converts this list into a **pandas DataFrame**, producing a unified dataset of comments ready for analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments from video: PE2wHqiwTf0\n",
      "Fetching comments from video: _QgIj7eNYAM\n",
      "Fetching comments from video: 1QUy3_Uofoo\n",
      "Fetching comments from video: tsgvHZr7vSQ\n",
      "Fetching comments from video: cLpQb-_c80Q\n",
      "Fetching comments from video: j2l1Ke0UtAE\n",
      "Fetching comments from video: MvVT08Eiml8\n",
      "Fetching comments from video: k-RcftDUUpw\n",
      "Fetching comments from video: r7T0k5QTKj8\n",
      "Fetching comments from video: tFjFBGMLj_M\n",
      "Fetching comments from video: OHQqgnq7Spc\n",
      "Fetching comments from video: CafjaZqnm6I\n",
      "Fetching comments from video: aMY5pQeGIE8\n",
      "Fetching comments from video: rQxPWdhoQe0\n",
      "Fetching comments from video: _sr2wpO4b64\n",
      "Fetching comments from video: 5DZlA-j__5E\n",
      "Fetching comments from video: X67GxburFjQ\n",
      "Fetching comments from video: BdweCZI0bGo\n",
      "Fetching comments from video: RTVZVv2Xnv8\n",
      "Fetching comments from video: SMQ9jIypeOA\n",
      "Fetching comments from video: aFcgKYUM_Yk\n",
      "Fetching comments from video: VzmgTg4g2Ng\n",
      "Fetching comments from video: wk1w_U1Xmcw\n",
      "Fetching comments from video: rq3aUwY859U\n",
      "Fetching comments from video: 30ROx6lbBVs\n",
      "Fetching comments from video: My-2-MyxamQ\n",
      "Skipping video My-2-MyxamQ (comments disabled or restricted)\n",
      "Fetching comments from video: 4FUu1uCxEWI\n",
      "Fetching comments from video: Lyqo2uJvNO4\n",
      "Fetching comments from video: vXU4HdbBtmo\n",
      "Fetching comments from video: HHujfrRiWEI\n",
      "Fetching comments from video: smJiCdWtgHk\n",
      "Fetching comments from video: wMEauuKV0BU\n",
      "Fetching comments from video: UJOYzLMq9Mk\n",
      "Fetching comments from video: QHXET1G9Y5U\n",
      "Fetching comments from video: i0Bx7ioky5g\n",
      "Fetching comments from video: Tg-3knE_nAM\n",
      "Fetching comments from video: VCsy9BO6_3g\n",
      "Fetching comments from video: 7I4_JCgyygU\n",
      "Fetching comments from video: uI3dTs1aB1s\n",
      "Fetching comments from video: X9HLsnzp9Os\n",
      "Fetching comments from video: DJqJ755MU84\n",
      "Fetching comments from video: wzOGAQhFpew\n",
      "Fetching comments from video: K_pnrP93_98\n",
      "Fetching comments from video: o1-BUCdog1c\n",
      "Fetching comments from video: PKK58ypgvOE\n",
      "Fetching comments from video: HTOxxCRqpLM\n",
      "Fetching comments from video: nXgp5fQbeKI\n",
      "Fetching comments from video: vYz5Pq8vnpE\n",
      "Fetching comments from video: 4kcuj00IfII\n",
      "Fetching comments from video: Fz5ybu9wOG4\n",
      "Fetching comments from video: 5nKetg9TSjQ\n",
      "Fetching comments from video: Zo43WjH7alE\n",
      "Fetching comments from video: q6X7BAkEy-Q\n",
      "Fetching comments from video: FPG5SVb89KA\n",
      "Fetching comments from video: vXXiVcVM_2w\n",
      "Fetching comments from video: gjNOm4_IAZU\n",
      "Fetching comments from video: uiQN8fmWYGo\n",
      "Fetching comments from video: ZjwmnXzfKHs\n",
      "Fetching comments from video: TJR9GWi1DZk\n",
      "Fetching comments from video: zR2Zq5jCRD4\n",
      "Fetching comments from video: _Ng2egpbA7Y\n",
      "Fetching comments from video: fjVnR5c0xZ4\n",
      "Fetching comments from video: nj2c5mKGTtQ\n",
      "Fetching comments from video: pP-TQwDOGko\n",
      "Fetching comments from video: -Uwcl9UsO9g\n",
      "Fetching comments from video: 6p1m2nCE7jE\n",
      "Fetching comments from video: hm61mi9ZGvI\n",
      "Fetching comments from video: A5u1xZwClvU\n",
      "Fetching comments from video: zrgUJSs_jzg\n",
      "Fetching comments from video: HyMo7vBE1IQ\n",
      "Fetching comments from video: Pyrp2LXpYSo\n",
      "Fetching comments from video: sJaFqiMfyM0\n",
      "Fetching comments from video: cfAZAFQp1S0\n",
      "Fetching comments from video: xP5nOMNku6w\n",
      "Fetching comments from video: kTDGNy5tib4\n",
      "Fetching comments from video: mhcEARrOTLQ\n",
      "Fetching comments from video: xNaqrXmsQZo\n",
      "Fetching comments from video: ljY7y7HYP3M\n",
      "Fetching comments from video: qb_SAXtQ6eQ\n",
      "Fetching comments from video: I3smXmpsorA\n",
      "Fetching comments from video: znAQ5nBPhWw\n",
      "Fetching comments from video: NLYauMMjcnM\n",
      "Fetching comments from video: BSoZw2BSxdg\n",
      "Skipping video BSoZw2BSxdg (comments disabled or restricted)\n",
      "Fetching comments from video: I_0v6RCYsw4\n",
      "Fetching comments from video: Va4IeSOJI8w\n",
      "Fetching comments from video: odqH3LZCahw\n",
      "Fetching comments from video: BPIxTS3GUXc\n",
      "Fetching comments from video: ga9QadHhbNU\n",
      "Fetching comments from video: 3khYEx0raIM\n",
      "Fetching comments from video: HlR4QB-UxAU\n",
      "Fetching comments from video: dB2xID8vTf8\n",
      "Fetching comments from video: UbrSQM5z-V8\n",
      "Fetching comments from video: MtX1C7clLjk\n",
      "Fetching comments from video: BvxiZXJPdog\n",
      "Fetching comments from video: umMZYN5jOA0\n",
      "Total comments collected: 10523\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "for vid in video_ids:\n",
    "    print(f\"Fetching comments from video: {vid}\")\n",
    "    comments = get_comments_from_video(vid, max_comments=200)\n",
    "\n",
    "    for comment in comments:\n",
    "        all_comments.append({\n",
    "            \"video_id\": vid,\n",
    "            \"comment\": comment\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_comments)\n",
    "\n",
    "print(\"Total comments collected:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Filter Comments by Relevance Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters the DataFrame to retain only comments containing **workplace-related keywords**, using a case-insensitive check. The resulting dataset contains **relevant comments** for further sentiment or topic analysis, reducing noise from unrelated content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining comments: 4659\n"
     ]
    }
   ],
   "source": [
    "# Define a list of keywords relevant to workplace discussions\n",
    "keywords = [\n",
    "    \"work\", \"job\", \"manager\", \"salary\", \"culture\",\n",
    "    \"toxic\", \"career\", \"stress\", \"environment\",\n",
    "    \"boss\", \"employee\", \"promotion\", \"office\",\n",
    "    \"quit\", \"fired\", \"workload\", \"shift\"\n",
    "]\n",
    "\n",
    "def is_relevant(comment):\n",
    "    comment_lower = comment.lower()\n",
    "    return any(word in comment_lower for word in keywords)\n",
    "\n",
    "df = df[df[\"comment\"].apply(is_relevant)]\n",
    "print(\"Remaining comments:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Remove Duplicate Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code eliminates duplicate comments from the DataFrame by checking the `\"comment\"` column, ensuring each comment is unique. It also reports how many duplicates were removed, providing a **cleaned dataset** ready for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 17\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df)\n",
    "\n",
    "df = df.drop_duplicates(subset=\"comment\")\n",
    "\n",
    "print(\"Removed duplicates:\", initial_count - len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Filter Short and Spam/Promotional Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out **short comments (<6 words)** and **spam/promotional content** containing URLs or keywords like “subscribe” or “channel,” producing a **cleaner, high-quality dataset** for analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comments with fewer than 6 words\n",
    "df = df[df[\"comment\"].str.split().str.len() > 5]\n",
    "\n",
    "# Remove comments containing URLs or common promotional terms\n",
    "df = df[~df[\"comment\"].str.contains(r\"http|www|subscribe|channel\", case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Sentiment Analysis Using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses **VADER sentiment analysis** to classify each comment as **positive, negative, or neutral** based on the compound score. The resulting `label` column allows for **quantitative sentiment analysis** of the comment dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Positive    2240\n",
       "Negative    1730\n",
       "Neutral      484\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to assign sentiment label based on compound score\n",
    "def label_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    \n",
    "    if score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment labeling to all comments\n",
    "df[\"label\"] = df[\"comment\"].apply(label_sentiment)\n",
    "\n",
    "# Display counts of each sentiment category\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentiment labeling is done using VADER, a lexicon and rule-based sentiment analyzer designed for social media text. \n",
    "2. It classifies comments as positive, negative, or neutral based on a compound score. \n",
    "3. Alternative approaches include manual hand-labeling (more accurate but time-consuming) or using pre-labeled datasets. \n",
    "4. VADER is chosen here because it is fast, effective for short informal text like YouTube comments, and requires no manual labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>The most gangster thing I've herd all year... ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>I feel bad for her coworkers who are probably ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Women are wired differently than men we need o...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>I think it's news to Amazon that she's an empl...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Beo I swear women get so much leniency on jobs...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>8-1pm is a big gap. Obviously Amazon doesn’t c...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>From my understanding she is flex, she can wor...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Amazon can be very lazy or not care. When I wo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Nope not Me bro my shift is 1:20 to 11:50</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>I was 5 minutes late to my shift once and they...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>This is typical for this generation I fired ab...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Ya....she doesnt work. She wouldnt be so high ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>I'm pretty tolerant with my team and have only...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Im happy i can show up pretty much whenever i ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Amazon must be desperate for employees.\\nMaybe...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>These are the types of people that expect $50/...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Why not work third shift if you aren't a morni...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>I used to work for a amazon warehouse. You can...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>Once worked with a dude that was great at his ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PE2wHqiwTf0</td>\n",
       "      <td>DUDE, IF I SHOWED UP FOR MY BOSS AT 1:00PM WHE...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id                                            comment     label\n",
       "1   PE2wHqiwTf0  The most gangster thing I've herd all year... ...  Negative\n",
       "9   PE2wHqiwTf0  I feel bad for her coworkers who are probably ...  Negative\n",
       "11  PE2wHqiwTf0  Women are wired differently than men we need o...  Positive\n",
       "12  PE2wHqiwTf0  I think it's news to Amazon that she's an empl...  Negative\n",
       "16  PE2wHqiwTf0  Beo I swear women get so much leniency on jobs...  Positive\n",
       "17  PE2wHqiwTf0  8-1pm is a big gap. Obviously Amazon doesn’t c...  Positive\n",
       "18  PE2wHqiwTf0  From my understanding she is flex, she can wor...  Positive\n",
       "20  PE2wHqiwTf0  Amazon can be very lazy or not care. When I wo...  Negative\n",
       "24  PE2wHqiwTf0          Nope not Me bro my shift is 1:20 to 11:50   Neutral\n",
       "27  PE2wHqiwTf0  I was 5 minutes late to my shift once and they...  Negative\n",
       "30  PE2wHqiwTf0  This is typical for this generation I fired ab...  Negative\n",
       "31  PE2wHqiwTf0  Ya....she doesnt work. She wouldnt be so high ...   Neutral\n",
       "33  PE2wHqiwTf0  I'm pretty tolerant with my team and have only...  Positive\n",
       "34  PE2wHqiwTf0  Im happy i can show up pretty much whenever i ...  Positive\n",
       "38  PE2wHqiwTf0  Amazon must be desperate for employees.\\nMaybe...  Negative\n",
       "39  PE2wHqiwTf0  These are the types of people that expect $50/...   Neutral\n",
       "40  PE2wHqiwTf0  Why not work third shift if you aren't a morni...  Positive\n",
       "42  PE2wHqiwTf0  I used to work for a amazon warehouse. You can...  Positive\n",
       "44  PE2wHqiwTf0  Once worked with a dude that was great at his ...  Positive\n",
       "46  PE2wHqiwTf0  DUDE, IF I SHOWED UP FOR MY BOSS AT 1:00PM WHE...  Negative"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and labeled comments to a CSV file\n",
    "df.to_csv(\"youtube_comments_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
