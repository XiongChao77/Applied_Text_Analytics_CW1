{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Google API client to fetch data (YouTube comments)\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Sentiment analysis library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fetch YouTube Video IDs Using Google API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script leverages the YouTube Data API v3 to perform programmatic video searches for specified queries, retrieving and aggregating the videoId of each result. It ensures a unique set of video IDs, forming a foundation for subsequent operations like comment scraping or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 92 videos down to 51 relevant results.\n",
      "videos saved to file: C:\\Users\\xc176\\Desktop\\lecture\\NLP\\Applied_Text_Analytics_CW1\\data\\amazon_filtered_videos.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up YouTube API Client\n",
    "API_KEY = \"AIzaSyCXQAtU3J_UDEt2XtY0NC_XJXP4SvDIJso\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "COMPANY = \"AMAZON\".lower()\n",
    "current_path = Path.cwd()\n",
    "data_dir = (current_path / \"..\" / \"data\").resolve()\n",
    "\n",
    "# Function: Search Videos\n",
    "def search_videos(query, max_results=15):\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        q=query,\n",
    "        type=\"video\",\n",
    "        maxResults=max_results,\n",
    "        order=\"relevance\"\n",
    "    )\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    for item in response[\"items\"]:\n",
    "        video_data = {\n",
    "            \"video_id\": item[\"id\"][\"videoId\"],\n",
    "            \"title\": item[\"snippet\"][\"title\"],\n",
    "            \"description\": item[\"snippet\"][\"description\"],  # Retain description for manual review\n",
    "            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "        }\n",
    "        videos.append(video_data)\n",
    "    return videos\n",
    "\n",
    "# Define queries\n",
    "queries = [\n",
    "    f\"{COMPANY} work culture\",\n",
    "    f\"why I quit job at {COMPANY}\",\n",
    "    f\"{COMPANY} employee review\",\n",
    "    f\"{COMPANY} toxic workplace\"\n",
    "]\n",
    "\n",
    "all_videos = []\n",
    "\n",
    "# Fetch videos for all queries\n",
    "for query in queries:\n",
    "    results = search_videos(query, max_results=30)\n",
    "    all_videos.extend(results)\n",
    "#Discard duplicate videos\n",
    "unique_videos = {video[\"video_id\"]: video for video in all_videos}\n",
    "unique_videos = list(unique_videos.values())\n",
    "\n",
    "#filter by title\n",
    "TITLE_FILTER_KEYWORDS = [\n",
    "    \"employee\",\n",
    "    \"working\",\n",
    "    \"work\",\n",
    "    \"warehouse\",\n",
    "    \"culture\",\n",
    "    \"conditions\",\n",
    "    \"experience\",\n",
    "    \"quit\"\n",
    "]\n",
    "\n",
    "def is_relevant_title(title):\n",
    "    title_lower = title.lower()\n",
    "\n",
    "    if COMPANY not in title_lower:\n",
    "        return False\n",
    "\n",
    "    #filter out videos that are not likely to be about employee experience, e.g. news, product reviews, etc.\n",
    "    for word in TITLE_FILTER_KEYWORDS:\n",
    "        pattern = r\"\\b\" + re.escape(word) + r\"\\b\"\n",
    "        if re.search(pattern, title_lower):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "filtered_videos = [video for video in unique_videos if is_relevant_title(video['title'])]\n",
    "print(f\"Filtered {len(unique_videos)} videos down to {len(filtered_videos)} relevant results.\")\n",
    "\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file_csv = data_dir / f\"{COMPANY}_filtered_videos.csv\"\n",
    "# Converting the list of dictionaries to a DataFrame\n",
    "df_videos = pd.DataFrame(filtered_videos)\n",
    "df_videos.to_csv(output_file_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"videos saved to file: {output_file_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Fetch Comments from a YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calls **YouTube CommentThreads API** to fetch top-level comments for a given videoId, paginating through results using list_next() until reaching max_comments. It handles errors gracefully if comments are disabled or restricted, returning a **list of plain-text comments** for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_video(video_id, max_comments=200):\n",
    "    comments = []\n",
    "\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "\n",
    "        while request and len(comments) < max_comments:\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comments.append(comment)\n",
    "\n",
    "            request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Skipping video {video_id} (comments disabled or restricted)\")\n",
    "        return []\n",
    "\n",
    "    return comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Aggregate Comments from Multiple Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over all fetched `video_ids`, calling `get_comments_from_video()` for each and storing the results with their corresponding video IDs in a **structured list**. It then converts this list into a **pandas DataFrame**, producing a unified dataset of comments ready for analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments from video: umMZYN5jOA0\n",
      "Fetching comments from video: 4FUu1uCxEWI\n",
      "Fetching comments from video: X9HLsnzp9Os\n",
      "Fetching comments from video: xNaqrXmsQZo\n",
      "Fetching comments from video: gjNOm4_IAZU\n",
      "Fetching comments from video: HHujfrRiWEI\n",
      "Fetching comments from video: ZjwmnXzfKHs\n",
      "Fetching comments from video: cfAZAFQp1S0\n",
      "Fetching comments from video: smJiCdWtgHk\n",
      "Fetching comments from video: o1-BUCdog1c\n",
      "Fetching comments from video: NLYauMMjcnM\n",
      "Fetching comments from video: I3smXmpsorA\n",
      "Fetching comments from video: uiQN8fmWYGo\n",
      "Fetching comments from video: BSoZw2BSxdg\n",
      "Skipping video BSoZw2BSxdg (comments disabled or restricted)\n",
      "Fetching comments from video: MvVT08Eiml8\n",
      "Fetching comments from video: HyMo7vBE1IQ\n",
      "Fetching comments from video: eBVy2-UQeOk\n",
      "Skipping video eBVy2-UQeOk (comments disabled or restricted)\n",
      "Fetching comments from video: bf0_aSvXpaA\n",
      "Fetching comments from video: VdoZYKGVblE\n",
      "Fetching comments from video: Fz5ybu9wOG4\n",
      "Fetching comments from video: wk1w_U1Xmcw\n",
      "Fetching comments from video: QHXET1G9Y5U\n",
      "Fetching comments from video: nXgp5fQbeKI\n",
      "Fetching comments from video: HlR4QB-UxAU\n",
      "Fetching comments from video: MtX1C7clLjk\n",
      "Fetching comments from video: 1QUy3_Uofoo\n",
      "Fetching comments from video: aMY5pQeGIE8\n",
      "Fetching comments from video: zrgUJSs_jzg\n",
      "Fetching comments from video: xP5nOMNku6w\n",
      "Fetching comments from video: Lyqo2uJvNO4\n",
      "Fetching comments from video: i0Bx7ioky5g\n",
      "Fetching comments from video: -Uwcl9UsO9g\n",
      "Fetching comments from video: tFjFBGMLj_M\n",
      "Fetching comments from video: ljY7y7HYP3M\n",
      "Fetching comments from video: UbrSQM5z-V8\n",
      "Fetching comments from video: BvxiZXJPdog\n",
      "Fetching comments from video: k-RcftDUUpw\n",
      "Fetching comments from video: dB2xID8vTf8\n",
      "Fetching comments from video: BPIxTS3GUXc\n",
      "Fetching comments from video: Pyrp2LXpYSo\n",
      "Fetching comments from video: vXXiVcVM_2w\n",
      "Fetching comments from video: PE2wHqiwTf0\n",
      "Fetching comments from video: X67GxburFjQ\n",
      "Fetching comments from video: PKK58ypgvOE\n",
      "Fetching comments from video: SMQ9jIypeOA\n",
      "Fetching comments from video: qb_SAXtQ6eQ\n",
      "Fetching comments from video: 4kcuj00IfII\n",
      "Fetching comments from video: odqH3LZCahw\n",
      "Fetching comments from video: tsgvHZr7vSQ\n",
      "Fetching comments from video: rq3aUwY859U\n",
      "Fetching comments from video: HTOxxCRqpLM\n",
      "Total comments collected: 5246\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "for video in filtered_videos:\n",
    "    vid = video[\"video_id\"]\n",
    "    print(f\"Fetching comments from video: {vid}\")\n",
    "    comments = get_comments_from_video(vid, max_comments=200)\n",
    "\n",
    "    for comment in comments:\n",
    "        all_comments.append({\n",
    "            \"video_id\": vid,\n",
    "            \"comment\": comment\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_comments)\n",
    "\n",
    "print(\"Total comments collected:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Filter Comments by Relevance Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters the DataFrame to retain only comments containing **workplace-related keywords**, using a case-insensitive check. The resulting dataset contains **relevant comments** for further sentiment or topic analysis, reducing noise from unrelated content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 5246 comments down to 2291 relevant comments.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of keywords relevant to workplace discussions\n",
    "keywords = [\n",
    "    \"work\", \"job\", \"manager\", \"salary\", \"culture\",\n",
    "    \"toxic\", \"career\", \"stress\", \"environment\",\n",
    "    \"boss\", \"employee\", \"promotion\", \"office\",\n",
    "    \"quit\", \"fired\", \"workload\", \"shift\"\n",
    "]\n",
    "\n",
    "def is_relevant(comment):\n",
    "    comment_lower = comment.lower()\n",
    "    return any(word in comment_lower for word in keywords)\n",
    "\n",
    "count_before = len(df)\n",
    "df = df[df[\"comment\"].apply(is_relevant)]\n",
    "print(f\"Filtered {count_before} comments down to {len(df)} relevant comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Remove Duplicate Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code eliminates duplicate comments from the DataFrame by checking the `\"comment\"` column, ensuring each comment is unique. It also reports how many duplicates were removed, providing a **cleaned dataset** ready for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 11\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df)\n",
    "\n",
    "df = df.drop_duplicates(subset=\"comment\")   ## Remove exact duplicates to improve model training, though this may affect true frequency distribution.\n",
    "\n",
    "print(\"Removed duplicates:\", initial_count - len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Filter Short and Spam/Promotional Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out **short comments (<6 words)** and **spam/promotional content** containing URLs or keywords like â€œsubscribeâ€ or â€œchannel,â€ producing a **cleaner, high-quality dataset** for analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed noise comments: 92\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df)\n",
    "\n",
    "# Remove comments with fewer than 6 words,reduce low-information noise\n",
    "df = df[df[\"comment\"].str.split().str.len() > 5]\n",
    "\n",
    "# Remove comments containing URLs or common promotional terms\n",
    "df = df[~df[\"comment\"].str.contains(r\"http|www|subscribe|channel\", case=False, regex=True)]\n",
    "print(\"Removed noise comments:\", initial_count - len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Sentiment Analysis Using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses **VADER sentiment analysis** to classify each comment as **positive, negative, or neutral** based on the compound score. The resulting `label` column allows for **quantitative sentiment analysis** of the comment dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------Positive--------------------------------------------------\n",
      "1. How do you guess amazon has reached to the very peak in the business world? Certainly not by hiring and keeping mediocre staff. Nobody is forced to work for or stay at amazon.\n",
      "2. Not Amazon, but working at a Chewy warehouse was probably just as bad as Amazon.. toxic work environment, where a lot of people did not do their fair share. When a new manager came there, he started finding issues with me and then set me up to fail.  \n",
      "They hire everyone off the street and then spit them out like numbers in a system. A lot of people had and I donâ€™t care attitude and made it hard for those who wanted to put in a lot of effort. It felt like a high school atmosphere. Management only came to you when you had â€œissuesâ€œ\n",
      "3. I donâ€™t think the general public understands just how big Amazon truly is. You have your product deliveries, Amazon prime video subscription, Amazon essentials, Amazon originals films, thereâ€™s Amazon web services that also has a good portion that does gov work as well, theyâ€™re getting into the space race with kuiper. Amazon is massive! Very easy to understand why the culture is very cutthroat.\n",
      "4. I work for Amazon Books. Ruthless standards. Love coaching plans and pips\n",
      "5. - **Amazon's relentless pursuit of perfection**:\n",
      "  - Compared to Microsoft's earlier years, Amazon is **ruthless** in seeking perfection, focusing on driving constant improvement and achieving market dominance.\n",
      "  - Amazon has **mastered the art of asking probing questions** and requires answers, pushing for concrete outcomes without allowing business processes to develop without direction.\n",
      "- **High-pressure work environment**:\n",
      "  - Working at Amazon is not considered a long-term career for most; **employees typically last around two years** due to the intense demands.\n",
      "  - **Mistakes are not tolerated**, and repeated errors lead to quick exits, reinforcing a culture where high performance is mandatory.\n",
      "- **Employee experiences**:\n",
      "  - Many professionals, even those with long careers at companies like Microsoft, **burn out quickly at Amazon**, feeling exhausted and uninterested in future work after short tenures.\n",
      "  - While **compensation is good**, the only recognition employees often receive is through pay, with little emphasis on praise or job satisfaction.\n",
      "- **Contrast with Microsoft**:\n",
      "  - Microsoft, in contrast, offers a more **relaxed and enjoyable work environment**, with recreational facilities like **foosball tables, gaming machines, and a cricket stadium**, promoting a balanced workplace culture.\n",
      "  - This congenial atmosphere leads to a **sustainable and less exhausting experience**, differing significantly from Amazon's high-pressure approach.\n",
      "--------------------------------------------------Neutral--------------------------------------------------\n",
      "1. he is so right, im quitting this month, exactly closing in on 2 years.\n",
      "2. It is a ruthless company. Beth Galetti, Head of HR allows CLT4 leaders to use high-powered microwaves energy against targeted employees and ex-employees. Satellite weapon is silent and invisible that causes Havana Syndrome. Nowadays, it has been spreading like a wild fire.\n",
      "3. Is this only in Technical and Wearhouse environment or also in Sales/BDM ?\n",
      "4. Amazon is a garbage company who lies and tricks people to get their way. Their cunning, despicable mentality of delivering the package much, much later than expected probes that they must get their way. That price of human garbage Jeff Bazos needs more money for his swimming pool while his poor, innocent customers donâ€™t get their package as they were so foolishly promised and his workers can barely afford a glass of water to keep them arrive. If you have something important youâ€™ve been expecting or anticipating, do yourself a favor by NOT ordering from Amazon, unless you looks high prices for late packages. I made that mistake and itâ€™ll be one that makes me more wise than I was before.\n",
      "5. Can Bezos do his warehouse Jobs or Delivery Jobs?  Put him to work there for a month.\n",
      "--------------------------------------------------Negative--------------------------------------------------\n",
      "1. Amazon is a terrible, bureaucratic place to work.\n",
      "2. I'm 51, been working since I was 15. The worst company and treatment that I've ever worked for\n",
      "3. Not only are they ruthless, they have a mandatory return policy. They recently screwed me out of a $52.88 food refund. They're packing and shipping department destroyed that part of my order. Employees in the packing and shipping department are stealing products out of people's food orders. I recently left Amazon as a Prime customer and I'll never go back.\n",
      "4. Unregrettable attrition, incompetent managers, unethical behavior,  weaponized performance reviews (PIP).  \n",
      "\n",
      "Unless you are desperate or are trying to fund your big house/two car/nanny lifestyle, avoid AMZ for the sake of your mental health and integrity.\n",
      "5. I worked at Amazon as sde, it was brutal. When I resigned it took me 4 months to fully recover from burnout ðŸ˜¢\n"
     ]
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Function to assign sentiment label based on compound score\n",
    "def label_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    \n",
    "    if score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment labeling to all comments\n",
    "df[\"label\"] = df[\"comment\"].apply(label_sentiment)\n",
    "\n",
    "# Display counts of each sentiment category\n",
    "df[\"label\"].value_counts()\n",
    "display_num = 5\n",
    "# Display top comments for each VADER sentiment category\n",
    "for sentiment in [\"Positive\", \"Neutral\", \"Negative\"]:\n",
    "    # print(f\"\\nTop {display_num} {sentiment} comments (VADER):\\n\")\n",
    "    # print(df[df[\"label\"] == sentiment][\"comment\"].head(display_num).to_string(index=False))\n",
    "    print(\"-\" * 50 + sentiment + \"-\" * 50)\n",
    "    comments = df[df[\"label\"] == sentiment][\"comment\"].head(display_num)\n",
    "    for i, comment in enumerate(comments, 1):\n",
    "        print(f\"{i}. {comment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentiment labeling is done using VADER, a lexicon and rule-based sentiment analyzer designed for social media text. \n",
    "2. It classifies comments as positive, negative, or neutral based on a compound score. \n",
    "3. Alternative approaches include manual hand-labeling (more accurate but time-consuming) or using pre-labeled datasets. \n",
    "4. VADER is chosen here because it is fast, effective for short informal text like YouTube comments, and requires no manual labeling.\n",
    "\n",
    "### Analysis based on the example above:\n",
    "    -The misclassification rate appears higher in the Positive category than in the Negative category.\n",
    "    -Some long comments with clear emotional orientation were misclassified.\n",
    "    -A small number of comments were unrelated to the target company. VADER detects sentiment only; company-level relevance must be addressed during preprocessing.\n",
    "### Conclusion and Next Steps\n",
    "To obtain high-quality labelled data, relying solely on VADER may not be sufficient, and manual annotation may be more effective. However, considering the current labelling accuracy and time constraints, we will use the VADER-labelled data for the next stage of model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and labeled comments to a CSV file\n",
    "output_file_csv = data_dir / f\"{COMPANY}_vader_youtube_comments_cleaned.csv\"\n",
    "df.to_csv(output_file_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob is used for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement rate between VADER and TextBlob: 60.97%\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "comment   Negative  Neutral  Positive\n",
      "label                                \n",
      "Negative       460      101       248\n",
      "Neutral         39      105        75\n",
      "Positive       199      192       769\n",
      "--------------------------------------------------Positive--------------------------------------------------\n",
      "1. I donâ€™t think the general public understands just how big Amazon truly is. You have your product deliveries, Amazon prime video subscription, Amazon essentials, Amazon originals films, thereâ€™s Amazon web services that also has a good portion that does gov work as well, theyâ€™re getting into the space race with kuiper. Amazon is massive! Very easy to understand why the culture is very cutthroat.\n",
      "2. - **Amazon's relentless pursuit of perfection**:\n",
      "  - Compared to Microsoft's earlier years, Amazon is **ruthless** in seeking perfection, focusing on driving constant improvement and achieving market dominance.\n",
      "  - Amazon has **mastered the art of asking probing questions** and requires answers, pushing for concrete outcomes without allowing business processes to develop without direction.\n",
      "- **High-pressure work environment**:\n",
      "  - Working at Amazon is not considered a long-term career for most; **employees typically last around two years** due to the intense demands.\n",
      "  - **Mistakes are not tolerated**, and repeated errors lead to quick exits, reinforcing a culture where high performance is mandatory.\n",
      "- **Employee experiences**:\n",
      "  - Many professionals, even those with long careers at companies like Microsoft, **burn out quickly at Amazon**, feeling exhausted and uninterested in future work after short tenures.\n",
      "  - While **compensation is good**, the only recognition employees often receive is through pay, with little emphasis on praise or job satisfaction.\n",
      "- **Contrast with Microsoft**:\n",
      "  - Microsoft, in contrast, offers a more **relaxed and enjoyable work environment**, with recreational facilities like **foosball tables, gaming machines, and a cricket stadium**, promoting a balanced workplace culture.\n",
      "  - This congenial atmosphere leads to a **sustainable and less exhausting experience**, differing significantly from Amazon's high-pressure approach.\n",
      "3. In the customer's stance of point of view, this business operation is magnificent. If from higher ups to base level jobs, were given more chances to make mistakes, it would affect millions of customers. One mistake there could potentially affect millions of customers and millions of dollars regarding investors. Everyone who complains still uses amazon, whether directly or indirectly. No company has done what amazon has, and because of their bar performance, the pressure of success would obviously be something never seen or endured until now.\n",
      "4. Was in the military for 6 years. Can honestly say Amazon was the worst experience of my working life. Treated with absolute 0 respect or dignity. â€œStrive to be the worlds best employer.â€ What a load of horse shit\n",
      "5. he is so right, im quitting this month, exactly closing in on 2 years.\n",
      "--------------------------------------------------Neutral--------------------------------------------------\n",
      "1. Can someone tell me if your on a quota as a aws DCEO engineer and would you recommend it as a career job at aws.\n",
      "2. I work there I got them people scared asf Iâ€™ll beat wanna they ass\n",
      "3. Is this only in Technical and Wearhouse environment or also in Sales/BDM ?\n",
      "4. Amazon is Amazon because of its work culture. Have you ever tried to run a startup? Amazon is a giant startup and you are like a founder. Perspective changes everything\n",
      "5. Never knew Sylvester Stallone worked at amazon\n",
      "--------------------------------------------------Negative--------------------------------------------------\n",
      "1. How do you guess amazon has reached to the very peak in the business world? Certainly not by hiring and keeping mediocre staff. Nobody is forced to work for or stay at amazon.\n",
      "2. Not Amazon, but working at a Chewy warehouse was probably just as bad as Amazon.. toxic work environment, where a lot of people did not do their fair share. When a new manager came there, he started finding issues with me and then set me up to fail.  \n",
      "They hire everyone off the street and then spit them out like numbers in a system. A lot of people had and I donâ€™t care attitude and made it hard for those who wanted to put in a lot of effort. It felt like a high school atmosphere. Management only came to you when you had â€œissuesâ€œ\n",
      "3. Amazon is a terrible, bureaucratic place to work.\n",
      "4. I work for Amazon Books. Ruthless standards. Love coaching plans and pips\n",
      "5. I'm 51, been working since I was 15. The worst company and treatment that I've ever worked for\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to assign sentiment label using TextBlob polarity\n",
    "def label_textblob(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity  # polarity range: [-1, 1]\n",
    "    \n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply TextBlob sentiment labeling\n",
    "textblob_label  = df[\"comment\"].apply(label_textblob)\n",
    "agreement = (df[\"label\"] == textblob_label).mean()\n",
    "print(\"Agreement rate between VADER and TextBlob: {:.2%}\".format(agreement))\n",
    "conf_matrix = pd.crosstab(df[\"label\"], textblob_label)\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(conf_matrix)\n",
    "df[\"label\"] = textblob_label\n",
    "\n",
    "# Display counts of each sentiment category\n",
    "df[\"label\"].value_counts()\n",
    "# Display top  comments for each TextBlob sentiment category\n",
    "for sentiment in [\"Positive\", \"Neutral\", \"Negative\"]:\n",
    "    print(\"-\" * 50 + sentiment + \"-\" * 50)\n",
    "    comments = df[df[\"label\"] == sentiment][\"comment\"].head(display_num)\n",
    "    for i, comment in enumerate(comments, 1):\n",
    "        print(f\"{i}. {comment}\")\n",
    "# Save the cleaned and labeled comments to a CSV file\n",
    "output_file_csv = data_dir / f\"{COMPANY}_textblob_youtube_comments_cleaned.csv\"\n",
    "df.to_csv(output_file_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
