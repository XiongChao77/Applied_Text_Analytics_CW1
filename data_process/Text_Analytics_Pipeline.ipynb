{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XP_cJEA84QmF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code loads the YouTube comments dataset from the CSV file. The dataset contains three columns: video_id, comment and label. For classification we only use comment and label column. The dataset is then split into training and testing set using an 80/20 ratio. The split preserves the sentiment label distribution using `stratify=y` and ensures reproducibility with a fixed `random_state`,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S36A2Myw4Ws_"
      },
      "outputs": [],
      "source": [
        "CSV_PATH = r\"youtube_comments_cleaned.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "X = df[\"comment\"].astype(str).values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Text Preprocessing Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code defines a custom tokenizer that perfoms essential text cleaning steps: converting text to lowercase, extracting alphabetic tokens, removing stop-words and applying lemmatisation. These steps help reduce noise, standardise the text and ensure that different grammatical forms of a word are mapped to a common base form. By this we maintain full control over the preprocessing pipeline and ensure consistent text cleaning across all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MadwrdY4cVp",
        "outputId": "e1322593-b6c5-4e66-b1aa-fbf4942ad853"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_pattern = re.compile(r\"[A-Za-z]+\")\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    tokens = token_pattern.findall(text)\n",
        "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Model Evaluation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code function trains a given model, generates predictions on the test set and calculates accuracy, precision,recall and F1 score. `Macro averaging` is used to treat all classes equally which is important when the dataset is imbalanced. The function prints the results and returns them for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uqXkyX1v4fxZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"Accuracy       : {acc:.4f}\")\n",
        "    print(f\"Macro Precision: {precision:.4f}\")\n",
        "    print(f\"Macro Recall   : {recall:.4f}\")\n",
        "    print(f\"Macro F1       : {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": precision,\n",
        "        \"recall_macro\": recall,\n",
        "        \"f1_macro\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code defines four different ML pipeplines. The first two pipelines uses **CountVectorizer** which represents text as raw word counts, paired with **Multinomial Naive Bayes**. Next two pipeline uses **TF-IDF** which down weights common words, paired with **Logistic Regression**. Both unigram and bigram features are tested on each pipeline to evalute the imapct of n gram features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P8qc8Rvf5gdP"
      },
      "outputs": [],
      "source": [
        "#NB + Count (unigram)\n",
        "nb_count_uni = Pipeline([\n",
        "    (\"vect\", CountVectorizer(\n",
        "        tokenizer=custom_tokenizer,\n",
        "        ngram_range=(1, 1),\n",
        "        min_df=2\n",
        "    )),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "#NB + Count (bigram)\n",
        "nb_count_bi = Pipeline([\n",
        "    (\"vect\", CountVectorizer(\n",
        "        tokenizer=custom_tokenizer,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2\n",
        "    )),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "#LR + TF-IDF (unigram)\n",
        "lr_tfidf_uni = Pipeline([\n",
        "    (\"vect\", TfidfVectorizer(\n",
        "        tokenizer=custom_tokenizer,\n",
        "        ngram_range=(1, 1),\n",
        "        min_df=2\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(max_iter=300, n_jobs=-1))\n",
        "])\n",
        "\n",
        "#LR + TF-IDF (bigram)\n",
        "lr_tfidf_bi = Pipeline([\n",
        "    (\"vect\", TfidfVectorizer(\n",
        "        tokenizer=custom_tokenizer,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(max_iter=300, n_jobs=-1))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Training and Evaluating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code executes each of four pipelines and evaluates their performance using the defined evaluation function. The results are stored for later comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV7GCJRA5qm2",
        "outputId": "4c7e1253-49e7-4b18-dcc2-81aad80fc8f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NB + Count (unigram):\n",
            "Accuracy       : 0.6476\n",
            "Macro Precision: 0.6376\n",
            "Macro Recall   : 0.4930\n",
            "Macro F1       : 0.4830\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NB + Count (bigram):\n",
            "Accuracy       : 0.6453\n",
            "Macro Precision: 0.6283\n",
            "Macro Recall   : 0.4857\n",
            "Macro F1       : 0.4699\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR + TF-IDF (unigram):\n",
            "Accuracy       : 0.6768\n",
            "Macro Precision: 0.6598\n",
            "Macro Recall   : 0.5256\n",
            "Macro F1       : 0.5283\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR + TF-IDF (bigram):\n",
            "Accuracy       : 0.6667\n",
            "Macro Precision: 0.6681\n",
            "Macro Recall   : 0.5059\n",
            "Macro F1       : 0.5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "results.append(\n",
        "    evaluate_model(\"NB + Count (unigram)\", nb_count_uni, X_train, y_train, X_test, y_test)\n",
        ")\n",
        "results.append(\n",
        "    evaluate_model(\"NB + Count (bigram)\", nb_count_bi, X_train, y_train, X_test, y_test)\n",
        ")\n",
        "results.append(\n",
        "    evaluate_model(\"LR + TF-IDF (unigram)\", lr_tfidf_uni, X_train, y_train, X_test, y_test)\n",
        ")\n",
        "results.append(\n",
        "    evaluate_model(\"LR + TF-IDF (bigram)\", lr_tfidf_bi, X_train, y_train, X_test, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code sorts all model results by macro F1 score and prints concise comparison table. This makes it easy to identify the strongest model and understand different feature extraction methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_IjZKOxOK_",
        "outputId": "28eabf45-3ecb-4b28-974e-2c26f8e8e18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Comparison Table:\n",
            "\n",
            "                   model  accuracy  precision_macro  recall_macro  f1_macro\n",
            "2  LR + TF-IDF (unigram)  0.676768         0.659753      0.525595  0.528258\n",
            "3   LR + TF-IDF (bigram)  0.666667         0.668133      0.505935  0.500010\n",
            "0   NB + Count (unigram)  0.647587         0.637637      0.493007  0.482965\n",
            "1    NB + Count (bigram)  0.645342         0.628333      0.485695  0.469923\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"f1_macro\", ascending=False)\n",
        "\n",
        "print(\"\\nModel Comparison Table:\\n\")\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
